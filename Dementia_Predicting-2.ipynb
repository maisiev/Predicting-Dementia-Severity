#!/usr/bin/env python3
"""
Brain MRI Classification Training Script
Improved for stability on macOS
"""

# =====================
# macOS TensorFlow safety
# =====================
import os
os.environ["TF_METAL_DISABLE"] = "1"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import tensorflow as tf
import numpy as np
from tensorflow.keras import layers, Sequential, regularizers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (
    EarlyStopping,
    ReduceLROnPlateau,
    ModelCheckpoint,
)
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend for saving plots
import matplotlib.pyplot as plt
from datetime import datetime

# =====================
# Configuration
# =====================
PATH = "/Users/maisievarcoe/Desktop/AI/Coursework/images/Originals"
IMG_SIZE = (224, 224)
BATCH_SIZE = 8
EPOCHS = 80
SEED = 42

print("="*60)
print("BRAIN MRI CLASSIFICATION TRAINING")
print("="*60)
print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Data path: {PATH}")
print(f"Image size: {IMG_SIZE}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Max epochs: {EPOCHS}")
print("="*60 + "\n")

# =====================
# Detect classes and count samples
# =====================
print("Step 1: Detecting classes and counting samples...")
class_names = sorted(
    d for d in os.listdir(PATH)
    if os.path.isdir(os.path.join(PATH, d)) and not d.startswith('.')
)
num_classes = len(class_names)
print(f"  Found {num_classes} classes: {class_names}")

# Count samples per class
class_counts = {}
total_images = 0
for idx, class_name in enumerate(class_names):
    class_path = os.path.join(PATH, class_name)
    image_files = [f for f in os.listdir(class_path) 
                   if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')) 
                   and not f.startswith('.')]
    class_counts[idx] = len(image_files)
    total_images += len(image_files)
    print(f"  {class_name}: {len(image_files)} images")

# Calculate class weights
total_samples = sum(class_counts.values())
class_weight_dict = {}
for class_idx, count in class_counts.items():
    class_weight_dict[class_idx] = total_samples / (num_classes * count)

print(f"\n  Total images: {total_images}")
print(f"  Training split (~80%): {int(total_images * 0.8)}")
print(f"  Validation split (~20%): {int(total_images * 0.2)}")
print(f"  Class weights: {class_weight_dict}")
print("  ✓ Complete\n")

# =====================
# Load datasets
# =====================
print("Step 2: Loading datasets...")
try:
    train_ds = tf.keras.utils.image_dataset_from_directory(
        PATH,
        labels="inferred",
        label_mode="int",
        color_mode="grayscale",
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        validation_split=0.2,
        subset="training",
        seed=SEED,
        shuffle=True,
    )
    
    val_ds = tf.keras.utils.image_dataset_from_directory(
        PATH,
        labels="inferred",
        label_mode="int",
        color_mode="grayscale",
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        validation_split=0.2,
        subset="validation",
        seed=SEED,
        shuffle=False,
    )
    print("  ✓ Datasets loaded successfully\n")
except Exception as e:
    print(f"  ✗ Error loading datasets: {e}")
    exit(1)

# =====================
# Data preprocessing and augmentation
# =====================
print("Step 3: Setting up data preprocessing...")

def preprocess_train(x, y):
    """Normalize and augment training data"""
    x = tf.cast(x, tf.float32) / 255.0
    # Data augmentation
    x = tf.image.random_flip_left_right(x)
    x = tf.image.random_brightness(x, 0.2)
    x = tf.image.random_contrast(x, 0.8, 1.2)
    return x, y

def preprocess_val(x, y):
    """Normalize validation data (no augmentation)"""
    x = tf.cast(x, tf.float32) / 255.0
    return x, y

# Apply preprocessing
train_ds = train_ds.map(preprocess_train, num_parallel_calls=2)
val_ds = val_ds.map(preprocess_val, num_parallel_calls=2)

# Optimize performance
train_ds = train_ds.prefetch(buffer_size=1)
val_ds = val_ds.prefetch(buffer_size=1)

print("  ✓ Preprocessing configured\n")

# =====================
# Build model
# =====================
print("Step 4: Building CNN architecture...")

model = Sequential([
    layers.Input(shape=(224, 224, 1)),
    
    # Block 1
    layers.Conv2D(32, 3, activation="relu", padding="same", 
                  kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    
    # Block 2
    layers.Conv2D(64, 3, activation="relu", padding="same",
                  kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.3),
    
    # Block 3
    layers.Conv2D(128, 3, activation="relu", padding="same",
                  kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.4),
    
    # Classifier
    layers.GlobalAveragePooling2D(),
    layers.Dense(64, activation="relu",
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation="softmax"),
], name="brain_mri_classifier")

model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

print("  Model architecture:")
model.summary()
print("  ✓ Model built successfully\n")

# =====================
# Setup callbacks
# =====================
print("Step 5: Configuring training callbacks...")

callbacks_list = [
    EarlyStopping(
        monitor="val_loss",
        patience=12,
        restore_best_weights=True,
        verbose=1,
    ),
    ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1,
    ),
    ModelCheckpoint(
        "best_model.h5",
        monitor="val_loss",
        save_best_only=True,
        verbose=1,
    ),
]

print("  ✓ Callbacks configured\n")

# =====================
# Train model
# =====================
print("="*60)
print("Step 6: Starting training...")
print("="*60 + "\n")

try:
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        class_weight=class_weight_dict,
        callbacks=callbacks_list,
        verbose=1,
    )
    
    print("\n" + "="*60)
    print("✓ TRAINING COMPLETED SUCCESSFULLY")
    print("="*60 + "\n")
    
except KeyboardInterrupt:
    print("\n\n⚠ Training interrupted by user")
    print("Saving current model state...")
    model.save("interrupted_model.h5")
    print("✓ Model saved as interrupted_model.h5")
    exit(0)
    
except Exception as e:
    print(f"\n\n✗ Training failed with error:")
    print(f"  {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# =====================
# Save final model
# =====================
print("Step 7: Saving final model...")
model.save("cnn_brain_model_final.h5")
print("  ✓ Model saved as cnn_brain_model_final.h5\n")

# =====================
# Plot training history
# =====================
print("Step 8: Generating training plots...")

plt.style.use('seaborn-v0_8-darkgrid')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy plot
ax1.plot(history.history['accuracy'], label='Training Accuracy', 
         linewidth=2, marker='o', markersize=3)
ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', 
         linewidth=2, marker='s', markersize=3)
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Accuracy', fontsize=12)
ax1.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)

# Loss plot
ax2.plot(history.history['loss'], label='Training Loss', 
         linewidth=2, marker='o', markersize=3)
ax2.plot(history.history['val_loss'], label='Validation Loss', 
         linewidth=2, marker='s', markersize=3)
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Loss', fontsize=12)
ax2.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
print("  ✓ Plot saved as training_history.png\n")

# =====================
# Print final metrics
# =====================
print("="*60)
print("FINAL TRAINING RESULTS")
print("="*60)

final_train_acc = history.history['accuracy'][-1]
final_val_acc = history.history['val_accuracy'][-1]
final_train_loss = history.history['loss'][-1]
final_val_loss = history.history['val_loss'][-1]

best_val_acc = max(history.history['val_accuracy'])
best_val_epoch = history.history['val_accuracy'].index(best_val_acc) + 1

print(f"Final Training Accuracy:    {final_train_acc:.4f} ({final_train_acc*100:.2f}%)")
print(f"Final Validation Accuracy:  {final_val_acc:.4f} ({final_val_acc*100:.2f}%)")
print(f"Final Training Loss:        {final_train_loss:.4f}")
print(f"Final Validation Loss:      {final_val_loss:.4f}")
print(f"Overfitting Gap:            {abs(final_train_acc - final_val_acc):.4f}")
print(f"\nBest Validation Accuracy:   {best_val_acc:.4f} ({best_val_acc*100:.2f}%) at epoch {best_val_epoch}")
print(f"Total Epochs Trained:       {len(history.history['accuracy'])}")
print("="*60)

print(f"\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("\n✓ All steps completed successfully!")
print("\nSaved files:")
print("  - best_model.h5 (best model based on validation loss)")
print("  - cnn_brain_model_final.h5 (final model)")
print("  - training_history.png (training plots)")
